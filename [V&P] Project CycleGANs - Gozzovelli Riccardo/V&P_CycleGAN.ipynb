{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"V&P_CycleGAN.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"LMc3H_z9X4Je","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NabMi8_9X69v","colab_type":"code","colab":{}},"source":["import time\n","from glob import glob\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import tensorflow as tf\n","from imageio import imread\n","from PIL import Image\n","import os\n","from functools import partial\n","from sklearn.model_selection import train_test_split\n","import random\n","import collections\n","\n","#Go to the right directory\n","os.chdir('/content/drive/My Drive/V&P')\n","print('Current directory %s ' % os.getcwd())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9e5V2sUcebZU","colab_type":"code","colab":{}},"source":["def conv2d(x, filter, kernel, stride, padding):\n","    return tf.layers.conv2d(inputs=x, filters=filter, kernel_size=kernel, strides=stride, padding=padding)\n","\n","def conv2dTranspose(x, filter, kernel, stride, padding):\n","    return tf.layers.conv2d_transpose(inputs=x, filters=filter, kernel_size=kernel, strides=stride, padding=padding, use_bias=False)\n","\n","def batchNormalization(x):\n","    return tf.layers.batch_normalization(inputs=x, axis=3, momentum=0.9, epsilon=1e-5)\n","\n","def instanceNormalization(x):\n","    return tf.contrib.layers.instance_norm(inputs=x, center=True, scale=True)\n","\n","def relu(x):\n","    return tf.nn.relu(features=x)\n","\n","def leakyRelu(x):\n","    return tf.nn.leaky_relu(features=x)\n","\n","def tanh(x):\n","    return tf.math.tanh(x=x)\n","\n","def sigmoid(x):\n","    return tf.math.sigmoid(x=x)\n","\n","def add(x,y):\n","    return tf.add(x,y)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wSh4TbL7YlNN","colab_type":"code","colab":{}},"source":["def residual_block(x):\n","    \"\"\"\n","    Residual block\n","    \"\"\"\n","    res = tf.pad(x, [ [0, 0], [1, 1], [1, 1], [0, 0] ], \"REFLECT\")\n","    res = relu( batchNormalization( conv2d(res, 256, 3, 1, \"valid\") ) )\n","    res = tf.pad(res, [ [0, 0], [1, 1], [1, 1], [0, 0] ], \"REFLECT\" )\n","    res = batchNormalization( conv2d(res, 256, 3, 1, \"valid\") )\n","    return add(res, x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KTnVTafMn2nc","colab_type":"code","colab":{}},"source":["def build_generator(image, A_or_B):\n","    \"\"\"\n","    Create a generator network using the hyperparameter values defined below\n","    \"\"\"\n","    residual_blocks = 9\n","    with tf.variable_scope(A_or_B+'_generator', reuse=tf.AUTO_REUSE):\n","        input = tf.pad(image, [[0, 0], [3, 3], [3, 3], [0, 0]], \"REFLECT\")\n","    \n","        # First Convolution block\n","        h_conv1 = relu( instanceNormalization( conv2d(input, 64, 7, 1, 'valid') ) )\n","        \n","        # 2nd Convolution block\n","        h_conv2 = relu( instanceNormalization( conv2d(h_conv1, 128, 3, 2, 'same') ) )\n","\n","        # 3rd Convolution block\n","        h_conv3 = relu( instanceNormalization( conv2d(h_conv2, 256, 3, 2,'same') ) )\n","        \n","        # 9 Residual blocks\n","        for i in range(1, residual_blocks+1):\n","            residual = residual_block(h_conv3)\n","    \n","        # Upsampling blocks\n","        # 1st Upsampling block\n","        h_conv4 = relu( instanceNormalization( conv2dTranspose(residual, 128, 3, 2, 'same') ) )\n","    \n","        # 2nd Upsampling block\n","        h_conv5 = relu( instanceNormalization( conv2dTranspose(h_conv4, 64, 3, 2, 'same') ) )\n","\n","        # Last Convolution layer\n","        output = tanh( conv2d(h_conv5, 3, 7, 1, 'same') )\n","    \n","        return output  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UtPuPaeruAvA","colab_type":"code","colab":{}},"source":["def build_discriminator(image, A_or_B):\n","    \"\"\"\n","    Create a discriminator network using the hyperparameter values defined below\n","    \"\"\"\n","    #Add some gaussian noise to the discriminator\n","    image = image + tf.random_normal(shape=tf.shape(image), mean=0.0, stddev=0.1, dtype=tf.float32)\n","\n","    with tf.variable_scope(A_or_B + '_discriminator', reuse=tf.AUTO_REUSE ):\n","        # 1st Convolutional block\n","        h_conv1 = leakyRelu(conv2d(image, 64, 4, 2, \"same\"))\n","\n","        # 3 Hidden Convolution blocks\n","        hidden_conv1 = leakyRelu(instanceNormalization(conv2d(h_conv1, 128, 4, 2, \"same\")))\n","        hidden_conv2 = leakyRelu(instanceNormalization(conv2d(hidden_conv1, 256, 4, 2, \"same\")))\n","        hidden_conv3 = leakyRelu(instanceNormalization(conv2d(hidden_conv2, 512, 4, 2, \"same\")))\n","    \n","        # Last Convolution layer\n","        output = conv2d(hidden_conv3, 1, 4, 1, \"same\")\n","        \n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3EDWoY2U4Inp","colab_type":"code","colab":{}},"source":["def load_images(data_dirA, data_dirB):\n","    count = 0\n","    percentage = 10\n","    \n","    if not data_dirA == data_dirB:\n","        imagesA = glob(data_dirA+ '/*.*')\n","        imagesB = glob(data_dirB+ '/*.*')\n","        value = 33\n","    else:\n","        imagesA = glob(data_dirA+ '/trainA/*.*')\n","        imagesB = glob(data_dirA+ '/trainB/*.*')\n","        value = 107\n","\n","    allImagesA = []\n","    allImagesB = []\n","\n","    for index, filename in enumerate(imagesB):\n","        if count % value == 0:\n","            print(\"Stored %d%% images\" %percentage)\n","            percentage += 10\n","        imgA = imread(imagesA[index], pilmode='RGB')\n","        imgB = imread(filename, pilmode='RGB')\n","        \n","        imgA = np.array(Image.fromarray(imgA).resize((256, 256)) )\n","        imgB = np.array(Image.fromarray(imgB).resize((256, 256)) )\n","\n","        if np.random.random() > 0.5:\n","            imgA = np.fliplr(imgA)\n","            imgB = np.fliplr(imgB)\n","\n","        allImagesA.append(imgA)\n","        allImagesB.append(imgB)\n","        count += 1\n","\n","    # Normalize images\n","    allImagesA = np.array(allImagesA) / 127.5 - 1.\n","    allImagesB = np.array(allImagesB) / 127.5 - 1.\n","\n","    return allImagesA, allImagesB"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1mWO5e27Ecf","colab_type":"code","colab":{}},"source":["def load_images_test(data_dirA, data_dirB):\n","    count = 0\n","    percentage = 10\n","    \n","    imagesA = glob(data_dirA+ '/testA/*.*')\n","    imagesB = glob(data_dirA+ '/testB/*.*')\n","    print(len(imagesA))\n","    print(len(imagesB))\n","    allImagesA = []\n","    allImagesB = []\n","\n","    for index, filename in enumerate(imagesB):\n","        imgA = imread(imagesA[index], pilmode='RGB')\n","        imgB = imread(filename, pilmode='RGB')\n","        \n","        imgA = np.array(Image.fromarray(imgA).resize((256, 256)) )\n","        imgB = np.array(Image.fromarray(imgB).resize((256, 256)) )\n","\n","        if np.random.random() > 0.5:\n","            imgA = np.fliplr(imgA)\n","            imgB = np.fliplr(imgB)\n","\n","        allImagesA.append(imgA)\n","        allImagesB.append(imgB)\n","        count += 1\n","\n","    # Normalize images\n","    allImagesA = np.array(allImagesA) / 127.5 - 1.\n","    allImagesB = np.array(allImagesB) / 127.5 - 1.\n","\n","    return allImagesA, allImagesB"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3dxakUO8gQZ","colab_type":"code","colab":{}},"source":["def load_test_batch(data_dir, batch_size):\n","    imagesA = glob(data_dir + '/testA/*.*')\n","    imagesB = glob(data_dir + '/testB/*.*')\n","\n","    imagesA = np.random.choice(imagesA, batch_size)\n","    imagesB = np.random.choice(imagesB, batch_size)\n","\n","    allA = []\n","    allB = []\n","\n","    for index, filename in enumerate(imagesA):\n","        # Load images and resize images\n","        imgA = np.array(Image.fromarray(imread(filename, pilmode='RGB')).resize((256, 256)))\n","        imgB = np.array(Image.fromarray(imread(imagesB[index], pilmode='RGB')).resize((256, 256)))\n","\n","        allA.append(imgA)\n","        allB.append(imgB)\n","\n","    return np.array(allA) / 127.5 - 1.0, np.array(allB) / 127.5 - 1.0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g8Ityo9t8kFQ","colab_type":"code","colab":{}},"source":["def save_images(originalA, generatedB, reconstructedA, originalB, generatedA, reconstructedB, path, predicting):\n","    \"\"\"\n","    Save images\n","    \"\"\"\n","    if predicting:\n","        #Disable interactive plotting when predicting\n","        plt.ioff()\n","    \n","    fig = plt.figure(figsize=(7,7))\n","    ax = fig.add_subplot(2, 3, 1)\n","    ax.imshow(originalA)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Original\")\n","    if predicting:\n","        #Close fig\n","        plt.close(fig)\n","\n","    ax = fig.add_subplot(2, 3, 2)\n","    ax.imshow(generatedB)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Generated\")\n","\n","    ax = fig.add_subplot(2, 3, 3)\n","    ax.imshow(reconstructedA)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Reconstructed\")\n","\n","    ax = fig.add_subplot(2, 3, 4)\n","    ax.imshow(originalB)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Original\")\n","\n","    ax = fig.add_subplot(2, 3, 5)\n","    ax.imshow(generatedA)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Generated\")\n","\n","    ax = fig.add_subplot(2, 3, 6)\n","    ax.imshow(reconstructedB)\n","    ax.axis(\"off\")\n","    ax.set_title(\"Reconstructed\")\n","\n","    plt.savefig(path)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hktgqXL-fLiC","colab_type":"code","colab":{}},"source":["def add_summary(writer, name, value, global_step):\n","    summary = tf.Summary(value=[tf.Summary.Value(tag=name, simple_value=value)])\n","    writer.add_summary(summary, global_step=global_step)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L7q4L13yge33","colab_type":"code","colab":{}},"source":["def CycleGAN():\n","    \n","    generatorA2B = partial(build_generator, A_or_B='AtoB')\n","    generatorB2A = partial(build_generator, A_or_B='BtoA')\n","    discriminatorA = partial(build_discriminator, A_or_B='A')\n","    discriminatorB = partial(build_discriminator, A_or_B='B')\n","    \n","    real_imageA = tf.placeholder(\"float\", shape=[None, 256, 256, 3], name=\"Real_Image_A\")    \n","    real_imageB = tf.placeholder(\"float\", shape=[None, 256, 256, 3], name=\"Real_Image_B\")\n","    \n","    fake_imageA = generatorB2A(real_imageB)\n","    fake_imageB = generatorA2B(real_imageA)\n","    reconstructedA = generatorB2A(fake_imageB)\n","    reconstructedB = generatorA2B(fake_imageA)\n","    \n","    probIsRealA = discriminatorA(real_imageA)\n","    probIsFakeA = discriminatorA(fake_imageA)\n","\n","    probIsRealB = discriminatorB(real_imageB)\n","    probIsFakeB = discriminatorB(fake_imageB)\n","    \n","    with tf.variable_scope('cyclic_loss'):\n","        g_loss_a_to_b = tf.losses.mean_squared_error(labels=probIsFakeB, predictions=tf.ones_like(probIsFakeA))\n","        g_loss_b_to_a = tf.losses.mean_squared_error(labels=probIsFakeA, predictions=tf.ones_like(probIsFakeA))\n","        cyc_loss_a = tf.losses.absolute_difference(real_imageA, reconstructedA)\n","        cyc_loss_b = tf.losses.absolute_difference(real_imageB, reconstructedB)       \n","        g_total_loss = g_loss_a_to_b + g_loss_b_to_a + cyc_loss_a * 10.0 + cyc_loss_b * 10.0\n","\n","    with tf.variable_scope(\"discriminator_A_loss\"):\n","        da_loss_real = tf.losses.mean_squared_error(labels=probIsRealA, predictions=tf.ones_like(probIsRealA))\n","        da_loss_b_to_a_fake = tf.losses.mean_squared_error(labels=probIsFakeA, predictions=tf.zeros_like(probIsFakeA))\n","        da_total_loss = da_loss_real + da_loss_b_to_a_fake\n","\n","    with tf.variable_scope(\"discriminator_B_loss\"):\n","        db_loss_real = tf.losses.mean_squared_error(labels=probIsRealB, predictions=tf.ones_like(probIsRealB))\n","        db_loss_a_to_b_sample = tf.losses.mean_squared_error(labels=probIsFakeB, predictions=tf.zeros_like(probIsFakeB))\n","        db_total_loss = db_loss_real + db_loss_a_to_b_sample\n","    \n","    with tf.variable_scope(\"train\"):     \n","        tvars = tf.trainable_variables()\n","        dA_vars = [var for var in tvars if 'A_discriminator' in var.name]\n","        dB_vars = [var for var in tvars if 'B_discriminator' in var.name]\n","        g_vars = [var for var in tvars if 'AtoB_generator' in var.name or 'BtoA_generator' in var.name]\n","\n","        adam = tf.train.AdamOptimizer(learning_rate=0.0002, beta1=0.5)\n","        trainerDA = adam.minimize(da_total_loss, var_list=dA_vars)\n","        trainerDB = adam.minimize(db_total_loss, var_list=dB_vars)\n","        trainerG = adam.minimize(g_total_loss, var_list=g_vars)\n","    \n","    return real_imageA, real_imageB, fake_imageA, fake_imageB, reconstructedA, reconstructedB, da_total_loss, db_total_loss, g_total_loss, trainerDA, trainerDB, trainerG"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HZiXbP5PHNGP","colab_type":"code","colab":{}},"source":["log_dir =  \"logs2/\"\n","checkpoint_save_path =  \"checkpoint2/model.ckpt\"\n","meta_graph_path = 'model2/model.ckpt.meta'\n","complete_model_path = 'model2/model.ckpt'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RAMV8UfJredL","colab_type":"code","colab":{}},"source":["dataset_path = \"data/summer2winter_yosemite\"\n","imagesA, imagesB = load_images(dataset_path, dataset_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3QKlwdphe1w","colab_type":"code","colab":{}},"source":["def batch_generator(A,B,batch_size):\n","    for start in range(0, len(A), batch_size):\n","        end = start + batch_size\n","        yield A[start:end], B[start:end]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgm2X1WzdsNm","colab_type":"code","colab":{}},"source":["epochs = 500\n","BATCH_SIZE = 32\n","n_iterations = int(np.ceil(len(imagesA)/1))\n","\n","tf.reset_default_graph()\n","g = tf.Graph()\n","with g.as_default():  \n","    real_imageA, real_imageB, fake_imageA, fake_imageB, reconstructedA, reconstructedB, da_total_loss, db_total_loss, g_total_loss, trainerDA, trainerDB, trainerG = CycleGAN()\n","    saver = tf.train.Saver() \n","    \n","with tf.Session(graph=g) as sess:\n","    if tf.train.latest_checkpoint('checkpoint2/'):\n","        print(\"Checkpoint present. Restoring model.\")\n","        saver.restore(sess, tf.train.latest_checkpoint('checkpoint2/'))\n","    else:\n","        print(\"Model not present. Initializing variables.\")\n","        sess.run(tf.local_variables_initializer())\n","        sess.run(tf.global_variables_initializer()) \n","    print(\"\\nStarting training...\")    \n","    train_writer = tf.summary.FileWriter(log_dir, sess.graph)\n","    try:\n","        for epoch in range(81, epochs):\n","            print(\"\\nEpoch\", epoch + 1)\n","            epoch_da_loss, epoch_db_loss, epoch_g_loss = 0., 0., 0.\n","            mb = 0\n","            start = time.perf_counter()\n","            print(\"=======\"*10)\n","            #for imageA, imageB in batch_generator(imagesA, imagesB, BATCH_SIZE):\n","            for imageA, imageB in zip(imagesA, imagesB):\n","                imageA = np.reshape(imageA, [1, 256,256,3])\n","                imageB = np.reshape(imageB, [1, 256, 256, 3])\n","                mb += 1\n","                #Retrieve the output of the two generators (A and B)\n","                A_Fake, B_Fake = sess.run([fake_imageA, fake_imageB], feed_dict={real_imageA:imageA, real_imageB:imageB})\n","                #Retrieve cyclic loss\n","                g_loss_val, _ = sess.run([g_total_loss, trainerG], feed_dict={real_imageA:imageA, real_imageB:imageB})\n","                epoch_g_loss += g_loss_val\n","                #Retrieve loss of discriminator A\n","                if epoch%2==0:\n","                    d_A_loss_val, _ = sess.run([da_total_loss, trainerDA], feed_dict={real_imageA:imageA, real_imageB:imageB})\n","                    epoch_da_loss += d_A_loss_val\n","                    #Retrieve loss of discriminator B\n","                    d_B_loss_val, _ = sess.run([db_total_loss, trainerDB], feed_dict={real_imageA:imageA, real_imageB:imageB})\n","                    epoch_db_loss += d_B_loss_val \n","            elapsed = time.perf_counter() - start\n","            print('Elapsed %.3f seconds. \\n' % elapsed)\n","            print(\"Cyclic Loss: {:.4f}\\tDiscriminator A Loss: {:.4f}\\tDiscriminator B Loss: {:.4f} \".format(epoch_g_loss/mb, epoch_da_loss/mb, epoch_db_loss/mb), end=\"\\r\")\n","            epoch_g_loss /= n_iterations\n","            epoch_da_loss /= n_iterations\n","            epoch_db_loss /= n_iterations\n","            add_summary(train_writer, \"epoch_g_loss\", epoch_g_loss, epoch)\n","            add_summary(train_writer, \"epoch_da_loss\", epoch_da_loss, epoch)\n","            add_summary(train_writer, \"epoch_db_loss\", epoch_db_loss, epoch)\n","            print(\"\\n\")\n","            print()\n","            print(\"=======\"*10)\n","            if epoch%10 == 0:\n","                # Save a checkpoint\n","                save_path = saver.save(sess, checkpoint_save_path)\n","                \n","                batchA, batchB = load_test_batch(data_dir=dataset_path, batch_size=2)\n","                # Try model so far\n","                #deqA = collections.deque(testA)\n","                #deqB = collections.deque(testB)\n","                #sampleA = np.asarray(random.sample(deqA, 2))\n","                #sampleB = np.asarray(random.sample(deqB, 2))\n","                inputA = g.get_tensor_by_name(\"Real_Image_A:0\")\n","                inputB = g.get_tensor_by_name(\"Real_Image_B:0\")\n","                # Get the generator tensors and their output \n","                fake_imageB = g.get_tensor_by_name('AtoB_generator/Tanh:0')\n","                fake_imageA = g.get_tensor_by_name('BtoA_generator/Tanh:0')\n","                fakeB, fakeA = sess.run([fake_imageB, fake_imageA], feed_dict= {real_imageA:batchA, real_imageB:batchB})\n","                # Get reconstructed images\n","                reconstructedA = g.get_tensor_by_name(\"BtoA_generator_1/Tanh:0\")\n","                reconstructedB = g.get_tensor_by_name(\"AtoB_generator_1/Tanh:0\")\n","                reconstructed_imageA, reconstructed_imageB = sess.run([reconstructedA, reconstructedB], feed_dict={real_imageA:batchA, real_imageB:batchB})\n","                # Shpw the generated and reconstructed images\n","                for i in range(0,2):\n","                    save_images(originalA=batchA[i], generatedB=fakeB[i], reconstructedA=reconstructed_imageA[i], originalB=batchB[i], generatedA=fakeA[i], reconstructedB=reconstructed_imageB[i], path=\"results/gen_{}_{}\".format(epoch, i), False)\n","    except KeyboardInterrupt:\n","        print(\"Keyboard interruption. Saving\")\n","        save_path = saver.save(sess, complete_model_path)\n","        train_writer.close()\n","    save_path = saver.save(sess, complete_model_path)\n","    train_writer.close()        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ch_RsWYZWM9h","colab_type":"text"},"source":["# **EVALUATION**"]},{"cell_type":"code","metadata":{"id":"9jjCNW5rUs1Y","colab_type":"code","colab":{}},"source":["log_dir =  \"logs2/\"\n","checkpoint_save_path =  \"checkpoint2/model.ckpt\"\n","meta_graph_path = 'model2/model.ckpt.meta'\n","complete_model_path = 'model2/model.ckpt'\n","dataset_path = \"data/summer2winter_yosemite\"\n","test_imagesA, test_imagesB = load_images_test(dataset_path, dataset_path)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"E2ytRg7Rpyhn","colab_type":"code","colab":{}},"source":["i = 0\n","with tf.Session() as sess:\n","            # Restore variables from disk.    \n","            saver = tf.train.import_meta_graph(meta_graph_path)\n","            graph = tf.get_default_graph()\n","            saver.restore(sess, complete_model_path)\n","            print(\"Model restored \\n\")\n","            for imageA, imageB in zip(test_imagesA, test_imagesB):\n","                i += 1\n","                imageA = np.reshape(imageA, [1, 256,256,3])\n","                imageB = np.reshape(imageB, [1, 256, 256, 3])\n","                # Get a batch of test data\n","                real_imageA = graph.get_tensor_by_name(\"Real_Image_A:0\")\n","                real_imageB = graph.get_tensor_by_name(\"Real_Image_B:0\")\n","                # Get the generator tensors and their output \n","                fake_imageB = graph.get_tensor_by_name('AtoB_generator/Tanh:0')\n","                fake_imageA = graph.get_tensor_by_name('BtoA_generator/Tanh:0')\n","                fakeB, fakeA = sess.run([fake_imageB, fake_imageA], feed_dict= {real_imageA:imageA, real_imageB:imageB})\n","                # Get reconstructed images\n","                reconstructedA = graph.get_tensor_by_name(\"BtoA_generator_1/Tanh:0\")\n","                reconstructedB = graph.get_tensor_by_name(\"AtoB_generator_1/Tanh:0\")\n","                reconstructed_imageA, reconstructed_imageB = sess.run([reconstructedA, reconstructedB], feed_dict={real_imageA:imageA, real_imageB:imageB})\n","                save_images(originalA=np.squeeze(imageA), generatedB=np.squeeze(fakeB), reconstructedA=np.squeeze(reconstructed_imageA), originalB=np.squeeze(imageB), generatedA=np.squeeze(fakeA), reconstructedB=np.squeeze(reconstructed_imageB), path=\"test_results/image_{}\".format(i), True)\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8yAbFWHhKB9j","colab_type":"code","colab":{}},"source":["%load_ext tensorboard\n","%tensorboard --logdir logs/"],"execution_count":0,"outputs":[]}]}